!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_OUTPUT_FILESEP	slash	/slash or backslash/
!_TAG_OUTPUT_MODE	u-ctags	/u-ctags or e-ctags/
!_TAG_PATTERN_LENGTH_LIMIT	96	/0 for no limit/
!_TAG_PROGRAM_AUTHOR	Universal Ctags Team	//
!_TAG_PROGRAM_NAME	Universal Ctags	/Derived from Exuberant Ctags/
!_TAG_PROGRAM_URL	https://ctags.io/	/official site/
!_TAG_PROGRAM_VERSION	0.0.0	//
ActionBound	envs/torch_envs.py	/^class ActionBound(NamedTuple):$/;"	c
Agent	models.py	/^class Agent(nn.Module):$/;"	c
CartPole	envs/cartpole.py	/^class CartPole:$/;"	c
DEBUG	config.py	/^DEBUG = sys.gettrace() is not None$/;"	v
EnvSpec	envs/torch_envs.py	/^class EnvSpec(NamedTuple):$/;"	c
F	models.py	/^from torch.nn import functional as F$/;"	x
Implementation of Stochastic Value gradients (WIP)	readme.md	/^## Implementation of Stochastic Value gradients (WIP)$/;"	s
OrnsteinUhlenbeckActionNoise	utils.py	/^class OrnsteinUhlenbeckActionNoise():$/;"	c
Pendulum	envs/pendulum.py	/^class Pendulum:$/;"	c
ReplayBuffer	buffer.py	/^class ReplayBuffer:$/;"	c
State	envs/pendulum.py	/^class State(typing.NamedTuple):$/;"	c
Transition	buffer.py	/^class Transition:$/;"	c
Ulqr	envs/lqr.py	/^class Ulqr():$/;"	c
Wrapper	envs/torch_envs.py	/^class Wrapper(gym.Wrapper):$/;"	c
__call__	utils.py	/^    def __call__(self) -> torch.tensor:$/;"	m	class:OrnsteinUhlenbeckActionNoise
__del__	envs/pendulum.py	/^    def __del__(self):$/;"	m	class:Pendulum
__init__	buffer.py	/^    def __init__(self, size):$/;"	m	class:ReplayBuffer
__init__	envs/cartpole.py	/^    def __init__(self, horizon):$/;"	m	class:CartPole
__init__	envs/lqr.py	/^    def __init__(self, horizon=200):$/;"	m	class:Ulqr
__init__	envs/pendulum.py	/^    def __init__(self, horizon=100):$/;"	m	class:Pendulum
__init__	envs/torch_envs.py	/^    def __init__(self, env, horizon, gamma=0.99):$/;"	m	class:Wrapper
__init__	models.py	/^    def __init__(self, obs_dim, action_dim, h_dim=100):$/;"	m	class:Agent
__init__	utils.py	/^    def __init__($/;"	m	class:OrnsteinUhlenbeckActionNoise
__iter__	buffer.py	/^    def __iter__(self):$/;"	m	class:Transition
__len__	buffer.py	/^    def __len__(self):$/;"	m	class:ReplayBuffer
__repr__	utils.py	/^    def __repr__(self) -> str:$/;"	m	class:OrnsteinUhlenbeckActionNoise
_dynamics	envs/lqr.py	/^        def _dynamics(state, action):$/;"	f	member:Ulqr.__init__	file:
_dynamics	envs/pendulum.py	/^        def _dynamics(obs, action):$/;"	f	member:Pendulum.__init__	file:
_encode_sample	buffer.py	/^    def _encode_sample(self, idxes):$/;"	m	class:ReplayBuffer
_get_action_bound	envs/torch_envs.py	/^def _get_action_bound(bound: gym.spaces.Box):$/;"	f
_obs_to_th	envs/pendulum.py	/^def _obs_to_th(obs):$/;"	f
_optimal_control	envs/lqr.py	/^    def _optimal_control(self, state):$/;"	m	class:Ulqr
_reward	envs/lqr.py	/^        def _reward(state, action):$/;"	f	member:Ulqr.__init__	file:
_reward	envs/pendulum.py	/^        def _reward(obs, action):$/;"	f	member:Pendulum.__init__	file:
_th_to_obs	envs/pendulum.py	/^def _th_to_obs(th, thdot):$/;"	f
_to_torch	envs/torch_envs.py	/^    def _to_torch(s, r, d):$/;"	m	class:Wrapper
action_dim	envs/pendulum.py	/^    action_dim = 1$/;"	v	class:Pendulum
action_size	envs/cartpole.py	/^    def action_size(self):$/;"	m	class:CartPole
action_size	envs/lqr.py	/^    def action_size(self):$/;"	m	class:Ulqr
action_size	envs/pendulum.py	/^    def action_size(self) -> int:$/;"	m	class:Pendulum
actor	svg.py	/^def actor(replay_buffer, agent, pi_optim, batch_size=32, epochs=1):$/;"	f
actor_epochs	config.py	/^actor_epochs = 1$/;"	v
angle_normalize	envs/pendulum.py	/^def angle_normalize(x):$/;"	f
append	buffer.py	/^    def append(self, transition: Transition):$/;"	m	class:ReplayBuffer
batch_size	config.py	/^batch_size = 64$/;"	v
buddy	main.py	/^import experiment_buddy as buddy$/;"	I
buffer_size	config.py	/^buffer_size = int(1e4)$/;"	v
close	envs/pendulum.py	/^    def close(self):$/;"	m	class:Pendulum
critic	svg.py	/^def critic(repay_buffer, agent, pi_optim, batch_size=32, gamma=0.99, epochs=10):$/;"	f
critic_epochs	config.py	/^critic_epochs = 5$/;"	v
critic_lr	config.py	/^critic_lr = 1e-3$/;"	v
eval_policy	eval_policy.py	/^def eval_policy(env, agent, log_dir, eval_runs=1):$/;"	f
f	envs/cartpole.py	/^        def f(state, action):$/;"	f	member:CartPole.__init__	file:
forward	models.py	/^    def forward(self, s):$/;"	m	class:Agent
gamma	config.py	/^gamma = 0.99$/;"	v
gather_grads	plot_grad_shift.py	/^gather_grads = []$/;"	v
gather_trajectory	main.py	/^def gather_trajectory(env, agent, replay_buffer, gamma=0.99):$/;"	f
generate_episode	eval_policy.py	/^def generate_episode(env, policy, max_steps=200):$/;"	f
get_action	models.py	/^    def get_action(self, s):$/;"	m	class:Agent
get_grad_norm	utils.py	/^def get_grad_norm(parameters):$/;"	f
grad_clip	config.py	/^grad_clip = 5.$/;"	v
grads	plot_grad_shift.py	/^    grads = model.weight.grad$/;"	v
h_dim	config.py	/^h_dim = 32$/;"	v
horizon	config.py	/^horizon = 200$/;"	v
host	config.py	/^    host = ""$/;"	v
host	config.py	/^    host = "mila"$/;"	v
l2_norm	utils.py	/^def l2_norm(parameters):$/;"	f
lambda_returns	utils.py	/^def lambda_returns(r_t: torch.Tensor, discount_t: torch.Tensor, v_t: torch.Tensor, lambda_: floa/;"	f
main	main.py	/^def main():$/;"	f
make_env	envs/torch_envs.py	/^def make_env(env_id="lqg", horizon=200):$/;"	f
max_steps	config.py	/^max_steps = int(1e5)$/;"	v
model	plot_grad_shift.py	/^model = nn.Linear(1,1,bias=False)$/;"	v
n_step_bootstrapped_returns	utils.py	/^def n_step_bootstrapped_returns(r_t: torch.Tensor, discount_t: torch.Tensor, v_t: torch.Tensor, /;"	f
nn	models.py	/^from torch import nn as nn$/;"	x
nn	plot_grad_shift.py	/^import torch.nn as nn$/;"	I
nn	utils.py	/^import torch.nn as nn$/;"	I
np	envs/cartpole.py	/^import numpy as np$/;"	I
np	envs/pendulum.py	/^import numpy as np$/;"	I
np	envs/torch_envs.py	/^import numpy as np$/;"	I
np	utils.py	/^import numpy as np$/;"	I
observation_size	envs/cartpole.py	/^    def observation_size(self):$/;"	m	class:CartPole
observation_size	envs/lqr.py	/^    def observation_size(self):$/;"	m	class:Ulqr
observation_size	envs/pendulum.py	/^    def observation_size(self) -> int:$/;"	m	class:Pendulum
optim	main.py	/^import torch.optim as optim$/;"	I
optim	plot_grad_shift.py	/^optim = torch.optim.SGD(params=model.parameters(),lr=1e-2)$/;"	v
policy_lr	config.py	/^policy_lr = 1e-3$/;"	v
proc_num	config.py	/^    proc_num = 1$/;"	v
proc_num	config.py	/^    proc_num = 5$/;"	v
q_loss	svg.py	/^def q_loss(value, s, a, r, s1, a1, done, gamma):$/;"	f
regularizer	config.py	/^regularizer = 1e-4$/;"	v
render	envs/cartpole.py	/^    def render(self, state, mode="human"):$/;"	m	class:CartPole
render	envs/lqr.py	/^    def render(self, state, mode='human'):$/;"	m	class:Ulqr
render	envs/pendulum.py	/^    def render(self, state, mode="human"):$/;"	m	class:Pendulum
render_policy	main.py	/^def render_policy(env, agent):$/;"	f
reset	envs/cartpole.py	/^    def reset(self,seed):$/;"	m	class:CartPole
reset	envs/lqr.py	/^    def reset(self, random_key):$/;"	m	class:Ulqr
reset	envs/pendulum.py	/^    def reset(self, seed):$/;"	m	class:Pendulum
reset	envs/torch_envs.py	/^    def reset(self, **kwargs):$/;"	m	class:Wrapper
reset	utils.py	/^    def reset(self) -> None:$/;"	m	class:OrnsteinUhlenbeckActionNoise
reward	envs/cartpole.py	/^        def reward(state, action):$/;"	f	member:CartPole.__init__	file:
rsample	models.py	/^    def rsample(self, s):$/;"	m	class:Agent
run	main.py	/^def run(env, agent, actor_optim, critic_optim, tb):$/;"	f
sample	buffer.py	/^    def sample(self, batch_size):$/;"	m	class:ReplayBuffer
save_every	config.py	/^save_every = 100$/;"	v
scalars_to_tb	main.py	/^def scalars_to_tb(writer, scalars, global_step):$/;"	f
seed	config.py	/^seed = 33$/;"	v
seed	envs/cartpole.py	/^    def seed(self, seed=None):$/;"	m	class:CartPole
should_render	config.py	/^    should_render = False$/;"	v
state_dim	envs/pendulum.py	/^    state_dim = 3$/;"	v	class:Pendulum
step	envs/cartpole.py	/^    def step(self, state, action):$/;"	m	class:CartPole
step	envs/lqr.py	/^    def step(self, state, action):$/;"	m	class:Ulqr
step	envs/pendulum.py	/^    def step(self, state: State, action: torch.tensor) -> State:$/;"	m	class:Pendulum
step	envs/torch_envs.py	/^    def step(self, action):$/;"	m	class:Wrapper
sweep_yaml	config.py	/^    sweep_yaml = ""  # k"sweep.yaml"$/;"	v
sweep_yaml	config.py	/^    sweep_yaml = "sweep.yaml"$/;"	v
td_loss	svg.py	/^def td_loss(value, s, r, s1, done, gamma):$/;"	f
torch_dist	envs/cartpole.py	/^import torch.distributions as torch_dist$/;"	I
train_horizon	config.py	/^train_horizon = 5$/;"	v
value	models.py	/^    def value(self, state, action):$/;"	m	class:Agent
weights_init	models.py	/^def weights_init(m):$/;"	f
x	plot_grad_shift.py	/^        x = y + xs[t+1] - y$/;"	v
x	plot_grad_shift.py	/^    x = model(x).detach()$/;"	v
x	plot_grad_shift.py	/^    x = x_0$/;"	v
x	plot_grad_shift.py	/^x  = x_0$/;"	v
x_0	plot_grad_shift.py	/^x_0 = torch.randn((1,1))$/;"	v
xs	plot_grad_shift.py	/^xs = [x_0]$/;"	v
y	plot_grad_shift.py	/^        y =  model(x)$/;"	v
